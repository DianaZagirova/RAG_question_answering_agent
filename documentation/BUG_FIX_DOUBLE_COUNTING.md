# Bug Fix: Double-Counting Validated Papers

## The Problem

The script `scripts/run_full_ingestion.py` was reporting **30,692 validated papers** when there are actually only **15,346 unique papers**.

### Root Cause

**Lines 124-133** (before fix):
```python
for doi, pmid in candidates:
    if doi:
        validated_dois.add(doi)
    if pmid:
        validated_pmids.add(str(pmid))

print(f"‚úì Found {len(validated_dois) + len(validated_pmids)} validated papers")
```

**The bug**: Almost every paper has BOTH a DOI and a PMID, so they were added to both sets and counted twice!

### Example
```
Paper: "Aging biomarkers study"
  - DOI: 10.1234/example
  - PMID: 12345678

Old logic:
  validated_dois.add("10.1234/example")     # Count 1
  validated_pmids.add("12345678")            # Count 2
  Total: 2 papers (WRONG!)

Correct logic:
  unique_papers.add("10.1234/example")      # Count 1
  Total: 1 paper (CORRECT!)
```

### Impact

- **Reported**: 30,692 papers to process
- **Actual**: 15,346 unique papers
- **Overestimate**: 2x (100% error!)

This caused:
- ‚ùå Incorrect time estimates (7 hours instead of 3.5 hours)
- ‚ùå Incorrect storage estimates (double)
- ‚ùå Confusion about progress

## The Fix

**New logic** (lines 124-140):
```python
# Track unique papers (not double-counting DOI+PMID)
unique_papers = set()
for doi, pmid in candidates:
    # Use DOI as primary identifier, fallback to PMID
    paper_id = doi if doi else str(pmid)
    unique_papers.add(paper_id)
    
    # Also add to separate sets for filtering
    if doi:
        validated_dois.add(doi)
    if pmid:
        validated_pmids.add(str(pmid))

print(f"‚úì Found {len(unique_papers):,} validated papers with full text")
```

**Key changes**:
1. ‚úÖ Track `unique_papers` set using DOI as primary ID
2. ‚úÖ Report count from `unique_papers` (not sum of dois + pmids)
3. ‚úÖ Still maintain separate sets for filtering logic
4. ‚úÖ Update all downstream calculations to use `unique_papers`

## Verification

```bash
python3 debug_validation_query.py
```

**Output**:
```
Query from run_full_ingestion.py:
   Results: 15,346 rows
   Unique DOIs: 15,346
   Unique PMIDs: 15,346
   Total unique identifiers: 30,692  ‚Üê OLD (WRONG)

Unique papers (COALESCE): 15,346    ‚Üê CORRECT

Papers with BOTH DOI and PMID: 15,348
Difference: 15,346 papers counted twice
```

## Corrected Numbers

### Before Fix
```
‚úì Found 30,692 validated papers with full text
‚úì Found 12,317 already ingested, 18,357 remaining
Estimated time: ~7 hours
```

### After Fix
```
‚úì Found 15,346 validated papers with full text
‚úì Found 12,232 already ingested, 3,114 remaining
Estimated time: ~1.5-2 hours
```

## Impact on Performance Estimates

### Original (Wrong) Estimates
- Papers to process: 18,357
- Time: ~7 hours
- Storage: ~8-10 GB

### Corrected Estimates
- Papers to process: ~3,114
- Time: ~1.5-2 hours (with GPU optimizations)
- Storage: ~2-3 GB

**Actual remaining work is 83% less than reported!** üéâ

## Files Modified

1. **scripts/run_full_ingestion.py**
   - Lines 124-140: Fixed double-counting logic
   - Lines 142-145: Use `unique_papers` for validation
   - Lines 178-185: Recalculate remaining with `unique_papers`
   - Lines 192-193: Use `unique_papers` for total count
   - Lines 211-215: Display correct breakdown

## Testing

Run with dry-run to verify:
```bash
python scripts/run_full_ingestion.py --validated-only --dry-run
```

Expected output:
```
‚úì Found 15,346 validated papers with full text
‚úì Found 12,232 already ingested, 3,114 remaining

üìö Papers:
  Validated with full text: 15,346
  Already ingested: 12,232
  Remaining to process: 3,114
```

## Summary

- ‚úÖ Bug fixed: No more double-counting
- ‚úÖ Correct paper counts: 15,346 (not 30,692)
- ‚úÖ Realistic estimates: ~1.5-2 hours (not 7 hours)
- ‚úÖ Accurate progress tracking
